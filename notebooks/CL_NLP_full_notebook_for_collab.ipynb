{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dWYnIrXKqOV"
   },
   "source": [
    "# Setup\n",
    "\n",
    "First let's install and setup the necessary libraries.\n",
    "\n",
    "First clone the github repository with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MV3AKSEPKqOy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!git clone https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020.git\n",
    "#With this command, the path to the data is \n",
    "workdir = './CrossLingual-NLP-AMLD2020/'\n",
    "os.environ[\"WORKDIR\"] = workdir\n",
    "#Please check if this correct, otherwise correct path_to_data\n",
    "!ls $WORKDIR/data/laser\n",
    "!mkdir $WORKDIR/data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38uYYYfCDz55"
   },
   "source": [
    "Download data from on your local file system and upload it to colab fs with the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilYMXGhaxIJf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!tar -jxf *.bz2 -C  $WORKDIR/data/raw/\n",
    "!rm ./semeveal15_sentiment_datasets.tar.bz2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8W3-_XJZFA9s"
   },
   "source": [
    "Install LASER and conceptNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksFtp-qvKqPO"
   },
   "outputs": [],
   "source": [
    "%cd CrossLingual-NLP-AMLD2020/\n",
    "!bash install_laser.sh\n",
    "!bash download_conceptNet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcegNHZPKqPo"
   },
   "source": [
    "Restart the runtime  environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4jhV-y5DHSF"
   },
   "outputs": [],
   "source": [
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKgzMiOYDPPD"
   },
   "source": [
    "Set  environnement variables and load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzJZ79Y4KqPu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "os.environ.setdefault(\"LASER\",\"/root/projects/LASER/\")\n",
    "assert os.environ.get('LASER'), 'Please set the environement variable LASER'\n",
    "LASER = os.environ['LASER']\n",
    "sys.path.append(LASER + 'source/lib')\n",
    "sys.path.append(LASER+\"source/\")\n",
    "\n",
    "workdir = './CrossLingual-NLP-AMLD2020/'\n",
    "os.environ[\"WORKDIR\"] = workdir\n",
    "sys.path.insert(1, workdir)\n",
    "\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vNOIVQOKqP3"
   },
   "source": [
    "If everything went well the following should not print any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EhaiKvfKqP5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.models import *\n",
    "\n",
    "print(Doc2Laser.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvnCwzJpKqQH"
   },
   "source": [
    "# Introduction to Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycmXbYllKqQK"
   },
   "source": [
    "## Language is hard! \n",
    "\n",
    "Take a look at the following sentences: \n",
    "1. Jane went to the store\n",
    "2. went the to Jane store \n",
    "3. Jane went store \n",
    "4. Jane goed store \n",
    "\n",
    "They (try to) express similar meanings, but some feel un-natural!  \n",
    "\n",
    "Several things to handle: \n",
    "- Morphology\n",
    "- Syntax <- touch on this \n",
    "- Semantics/World Knowledge <- touch on this but mostly shallow semantics\n",
    "- Discourse \n",
    "- Pragmatics \n",
    "- Multilinguality <- focus on this\n",
    "\n",
    "## Sentiment classification\n",
    "- binary (positive, negative)\n",
    "- ternary (positive, neutral, negative)\n",
    "- ordinal (image below!)\n",
    "\n",
    "<img src=\"../data/images/sentiment-5class.png?raw=1\" width=600>\n",
    "\n",
    "*Input* (x): a text span \n",
    "\n",
    "*Output* (y): a class/category (sentiment polarity in the sentiment classification example)\n",
    "\n",
    "**Goal**: Train a function $f(x) \\rightarrow y$\n",
    "\n",
    "- How to represent text? \n",
    "- What functions can we use for the task? \n",
    "- How to evaluate performance?\n",
    "\n",
    "\n",
    "## Machine learning workflow\n",
    "1. Get data\n",
    "1. Inspect the data\n",
    "1. Preprocess/Clean/Normalize the data\n",
    "1. Vector Representation\n",
    "1. Modeling \n",
    "1. Evaluation\n",
    "\n",
    "<img src=\"../data/images/pipeline.png?raw=1\" width=600>\n",
    "\n",
    "## Text representation: traditional bag-of-words\n",
    "Given a text, extract the vocabulary, build a vector of dim $|V|$, non-zeros are words that appear. \n",
    "<img src=\"../data/images/textVectorization.png?raw=1\" width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cGT-e6ZKqQN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE0-XzS9KqQT"
   },
   "source": [
    "- Words are identified by their ids\n",
    "- Non-zero means a word occurs\n",
    "- The value, is the number of times the word occurs in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQXI6Zb-KqQV"
   },
   "outputs": [],
   "source": [
    "vectorizer.transform(['This is the first document', 'is document the first this']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUtSI3qKKqQd"
   },
   "source": [
    "- order does not matter! Recall the example with Jane ;-)\n",
    "- words like 'and, the' matter the same with words like 'super, great, ..'. This is a limitation. \n",
    "- tf-idf (term frequence, inverse document frequency) is an heuristic that can get us far!\n",
    "\n",
    "$tf_{i,j}\\times\\log\\frac{N}{df_i}$\n",
    "\n",
    "where\n",
    "\n",
    "$tf_{i,j}$ is number of times the term $i$ appears in document $j$, $df_i$ is the document frequency in the full collection of documents and $N$ is the number of available documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqL2Nl-eKqQf"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eo2hvI49KqQm"
   },
   "source": [
    "Other tricks and tips: \n",
    "- Recall text is a sequence of symbols. We may care for characters instead of words (think typos) \n",
    "- We may care for longer sequences that single words: New York, not great, ..   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CB6cHvsKqQo"
   },
   "outputs": [],
   "source": [
    "# Character grams\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1)) # This creates character-grams of size 1\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn-X1KOUKqQx"
   },
   "source": [
    "N-grams are sequences of *objects*. Here, objects, are either charactets sequences or word sequences. For character sequences for example:\n",
    "\n",
    "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/ngrams.png?raw=1\">\n",
    "\n",
    "In this figure notice the sliding window of size 3. While moving from left to right, it generates the possible sequences that will be used to populate the vector representations. Due to the fact that the window is of size 3, the method will generate character 3-grams. If, instead of character in the figure, we were using words, we would be generating word 3-grams. \n",
    "\n",
    "**Question**: can you think of a limitation of word 3-grams, 4-grams, 5-grams etc.?\n",
    "\n",
    "**Exercise**: how to get these sequences in Python (in an elegant way)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRrqNh1NKqQ0"
   },
   "outputs": [],
   "source": [
    "# N-grams (can be either char-grams or word-grams)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2,3)) # This creates character-grams of sizes 2 and 3\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YcUkxRjKqQ5"
   },
   "source": [
    "## Dense Representations\n",
    "\n",
    "All of the above techniques have a common limitation. The do not encode semantics! \n",
    "This means that the vector for `amazing` is completely disimilar from the vector of `great` and the of vector `Laussane`. \n",
    "Can we do better?\n",
    "The answer is yes! Enter, word embeddings. \n",
    "Dense word representation, that can encode the meaning! \n",
    "\n",
    "<img src=\"../data/images/word2vec.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNJ8gO8IKqQ6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "# For more information of TSNE: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "# For more information on GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "vectors = open('../data/glove_excerpt.txt').read().strip().split('\\n')\n",
    "vectors = {line.split()[0]:np.array(line.split()[1:]).astype(float) for line in vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGquTwMSKqRB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Let's visualize this, using TSNE, a methods that can reduce the dimensionality of the vectors\n",
    "labels = list(vectors.keys())\n",
    "tokens = list(vectors.values())\n",
    "\n",
    "tsne_model = TSNE(perplexity=1.5, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "x = new_values[:,0]\n",
    "y = new_values[:,1]\n",
    "\n",
    "plt.figure(figsize=(7, 6)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lnoq9W0bKqRH"
   },
   "source": [
    "This is a great result for several reasons:\n",
    "- families of similar words are close between them\n",
    "- Some of them encode some syntax (magnificent and amazing) need similar vectors to approach their adverbs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Representations with Deep-NN Models\n",
    "\n",
    "More recently the introduction of deep neural models for building text representations provided us with capabilities of better language understading and subsequently solve easier text related tasks. Specifically, we can distinguish the so-called Transformers in three different classes with respect to the objective they optimize for: \n",
    "\n",
    "- Language Model: estimate the probability of a word given previous words.\n",
    "- Machine Translation: in a sequence mode predict the words in the target sentence.\n",
    "- Masked Language Model: predict the masked token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/images/lm_models.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnaY1-VKqRJ"
   },
   "source": [
    "# A Brief Introduction to Cross-Lingual Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHkm1AgiKqRN"
   },
   "source": [
    "We give here a brief introduction of the concepts and methods that can learn joint word embeddings for multiple languages.\n",
    "\n",
    "In the previous notebook we described how one can learn word embeddings from mono-lingual corpora. These mono-lingual word embeddings capture the particularities of the specific language but they cannot used in other languages. For example, we would like to leverage high resource languages like English in order to enable downstream tasks in low resource languages. To do so, one should learn word embeddings of different languages in a common space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFFcdUPDKqRO"
   },
   "source": [
    "Let's see how word embeddings learned in a common space look like. For this purpose we will use the ConceptNet multilingual embeddings for English and French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywh0cL6RKqRQ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils import load_embeddings,emb2numpy\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooB-uKbhKqRX"
   },
   "outputs": [],
   "source": [
    "# We select some English and French words\n",
    "english_words = [\"room\",\"hotel\",\"towel\",\"book\",\"coffee\",\"chair\",\"glass\",\"pen\",\"shoe\",\"two\",\"amazing\"]\n",
    "french_words = [\"hôtel\",\"chambre\",\"livre\",\"café\",\"chaise\",\"serviette\",\"verre\",\"stylo\",\"chaussure\",\"deux\",\"fantastique\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cqvyb3JXKqRa"
   },
   "outputs": [],
   "source": [
    "en_emb = load_embeddings(path=\"../concept_net_1706.300.en\", dimension=300,skip_header=False,vocab=english_words)\n",
    "fr_emb = load_embeddings(path=\"../concept_net_1706.300.fr\", dimension=300,skip_header=False,vocab=french_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsY5JTS2KqRd"
   },
   "outputs": [],
   "source": [
    "# Put the vectors in arrays\n",
    "words_en,V_en = emb2numpy(en_emb)\n",
    "words_fr,V_fr = emb2numpy(fr_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0xuDxZnKqRh"
   },
   "outputs": [],
   "source": [
    "vectors = np.concatenate((V_en,V_fr))\n",
    "all_words  = words_en+words_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_T_YWMMKqRl"
   },
   "outputs": [],
   "source": [
    "# We project the 300d vectors to a 2d space for visualization\n",
    "V_umap = UMAP(n_neighbors=3,min_dist=0.6).fit_transform(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd0I1LtSKqRo"
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "\n",
    "fig= plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
    "for i, word in enumerate(all_words):\n",
    "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12fc6__NKqRr"
   },
   "source": [
    "You can observe that the words from the different languages are close in the embeddings space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5V3bPXhdKqRs"
   },
   "source": [
    "Several methods for learning such cross-lingual embeddings have been proposed recently. They most straightforward ones try to align mono-lingual embeddings which has been learned seperately. In order to so, some sort of supervision is required which may be for example in the form of a bilingual dictionary or sentence-aligned data.\n",
    "\n",
    "The following figure presents schematically the approach of mono-lingual mapping.\n",
    "\n",
    "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/alignment.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLQpTmnCKqRt"
   },
   "source": [
    "The type of supervision can vary from parallel sentences, for example human translations, to cheaper signals like for example bilingual dictionaries.In the case of bilingual lexicons the respective methods learn linear projections from the target to the source embeddings using the dictionary. \n",
    "\n",
    "\n",
    "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/bilingual_alignement.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5uZmRruKqRv"
   },
   "source": [
    "Other recent methods do not require any seed dictionaries and induce in an iterative procedure one that is used to learn the projections. For a comprehensive study one can refer to [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7JSrGcJKqRy"
   },
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFJeyzIHKqRz"
   },
   "source": [
    "One issue with cross-lingual word embeddings is that they may not be able to capture salient information as they neglect linguistic dependecies. One should use sequence models, like RNNs,  on top of these representations in order to learn a vector representation of longer textual segments such as sentences. In a recent work presented by Facebook research, a multi-lingual model was developed that learns vector representations over sentences for 93 languages. [LASER](https://github.com/facebookresearch/LASER) (Language-Agnostic Sentence Representations) is essentially a translation model that leverages a seq2seq architecture (figure was taken from [2]).\n",
    "\n",
    "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/laser.png?raw=1)\n",
    "\n",
    "The model, unlike state-of-the-art translation models that use attention, uses a BiLSTM encoder with a max pooling operation which gives us the sentence embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhsUwnNKqR1"
   },
   "source": [
    "Let's use LASER and see how well can embed a few parallel sentences in English, French and Greek. For this, we will use the Doc2Laser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFLwkpPrKqR2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUqdUD-eKqR5"
   },
   "outputs": [],
   "source": [
    "print(Doc2Laser.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ts_HfX3KqR9"
   },
   "outputs": [],
   "source": [
    "en_sentences = [\"This is a nice hotel.\",\n",
    "                \"The bathroom was clean\",\n",
    "                \"The dog is brown\",\n",
    "                \"I will call you\",\n",
    "               \"Not very far from the center\"]\n",
    "\n",
    "# define a transformer\n",
    "doc2laser_transformer = Doc2Laser(\"en\")\n",
    "\n",
    "# Get the representation of the sentences\n",
    "X_en = doc2laser_transformer.transform(en_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0n9jzqOKqSC"
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\"Celui-ci était un hôtel magnifique\",\n",
    "                \"La salle de bain était propre\",\n",
    "                \"Le chien est brun\",\n",
    "                \"Je t'appelle\",\n",
    "               \"Pas très loin du centre\"]\n",
    "\n",
    "# Change the language in the transformer\n",
    "doc2laser_transformer.set_params(lang=\"fr\")\n",
    "X_fr = doc2laser_transformer.transform(fr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PssbZDwcKqSF"
   },
   "outputs": [],
   "source": [
    " # Change the language in the transformer\n",
    "doc2laser_transformer.set_params(lang=\"el\")\n",
    "gr_sentences = [\"Το ξενοδοχείο ήταν υπέροχο\",\n",
    "                \"Η τουαλέτα ήταν καθαρή\",\n",
    "                \"Ο σκύλος είναι καφέ\",\n",
    "                \"Σε παίρνω τηλέφωνο\",\n",
    "                \"Όχι πολύ μακριά από το κέντρο\"]\n",
    "X_gr = doc2laser_transformer.transform(gr_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_z2UrMGEKqSJ"
   },
   "source": [
    "Let's project the sentence representations now in a 2d space and check if the parallel sentences in the three languages are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vso0T9xpKqSN"
   },
   "outputs": [],
   "source": [
    "V_umap = UMAP(n_neighbors=5,min_dist=0.2).fit_transform(np.concatenate((X_en,X_fr,X_gr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvJFV-RnKqSj"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
    "for i, word in enumerate(en_sentences+fr_sentences+gr_sentences):\n",
    "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjeUc14SKqSq"
   },
   "source": [
    "We can observe that the parallel sentences are close to the embedding space which means that the model can capture the semantic in a single latent multi-lingual space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4gfSqOtKqSs"
   },
   "source": [
    "***Exercise:*** Try to add few more parallel sentence in other languages and project them with the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1468lh1kKqSw"
   },
   "source": [
    "## References\n",
    "\n",
    "[1. Ruder et al., A Survey Of Cross-lingual Word Embedding Models](https://arxiv.org/abs/1706.04902)\n",
    "\n",
    "[2. Artexte and Schwenk, Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)\n",
    "\n",
    "[3. Lena Voita et al., Evolution of Representations in the Transformer](https://arxiv.org/abs/1909.01380)\n",
    "\n",
    "[4. Balikas and Partalas, Wasserstein distances for evaluating cross-lingual embeddings](https://arxiv.org/abs/1910.11005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TiV2i9dKqS2"
   },
   "source": [
    "# Cross-lingual Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dp5AQlhRKqS5"
   },
   "source": [
    "## Problem description\n",
    "\n",
    "Cross-lingual document classification (CLDC) is the text mining problem where we are given:\n",
    "- labeled documents for training in a source language $\\ell_1$, and \n",
    "- test documents written in a target language $\\ell_2$. \n",
    "\n",
    "For example, the training documents are written in English, and the test documents are written in French. \n",
    "\n",
    "\n",
    "CLDC is an interesting problem. The hope is that we can use resource-rich languages to train models that can be applied to resource-deprived languages. This would result in transferring knowledge from one language to another. \n",
    "There are several methods that can be used in this context. In this workshop we start from naive approaches and progressively introduce more complex solutions. \n",
    "\n",
    "The most naive solution is to ignore the fact the training and test documents are written in different languages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0X2djheGKqS8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from collections import Counter\n",
    "from src.models import *\n",
    "from src.utils import *\n",
    "from src.dataset import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miKup2VyKqTG"
   },
   "source": [
    "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/classes.png?raw=1\" width=600>\n",
    "\n",
    "1. Dataset: holds the data of sources and target language\n",
    "2. System: This is a set of steps: Does fit, predict. Can be in the form of a pipeline also\n",
    "3. Experiment: Given a Dataset and a System it fits, predicts and reports evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdV-ysScKqTJ"
   },
   "source": [
    "For this workshop we will use a dataset from the [SemEval](http://alt.qcri.org/semeval2015/) workshop for the Sentiment Analysis task. While the tasks have three classes, that is **Positive, Negative, Neutral**, we will use only two classes in order to simplify it. So, let's load the data for a pair of languages and check a few statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q21T9mlMKqTM"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\"../data/raw/\",\"en\", \"es\")\n",
    "\n",
    "dataset.load_data()\n",
    "#To check the arguments of the function\n",
    "#print(dataset.load_cl_embeddings.__doc__)\n",
    "dataset.load_cl_embeddings(\"../\",300,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT-n2F8HKqTu"
   },
   "outputs": [],
   "source": [
    "# Plot the counts on the classes for the source language\n",
    "sns.countplot(dataset.y_train,order=[\"negative\",\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pXuQPcpKqT6"
   },
   "outputs": [],
   "source": [
    "# And for the Spanish dataset\n",
    "sns.countplot(dataset.y_test,order=[\"negative\",\"positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyY8IZ3uKqT_"
   },
   "source": [
    "Observe that the datasets are unbalanced as we have much more positive comments that negative ones. We will start by establishing a few baselines and see how we can improve over them by leveraging cross-lingual word embeddings. We will start with a dummy classifier that will respect the distribution of the classes to generate some random predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w56E2_mvKqUC"
   },
   "outputs": [],
   "source": [
    "# Let's keep the scores of all the expriments in a table\n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Model\", \"f-score\"]\n",
    "\n",
    "# Majority Class\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                     ('classifier', DummyClassifier(\"stratified\"))])\n",
    "runner = Runner(pipeline, dataset)\n",
    "score = runner.eval_system()\n",
    "x.add_row([\"Dummy\", format_score(score)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmYK4Ks7KqUR"
   },
   "source": [
    "We start with a model that just uses term frequencies in order to represent the documents. We expect that in cases where the source and target languages share a part of the vocabulary, for example in latin languages, this approach can potentially give descent results. We will just use unigrams for this exercice but of course you can alter this baseline in order to leverage character n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2s40EwIKqUh"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression on words\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True)), \n",
    "                     ('classifier', LogisticRegression(solver=\"lbfgs\"))])\n",
    "runner = Runner(pipeline, dataset)\n",
    "score = runner.eval_system()\n",
    "x.add_row([\"LR unigrams\",format_score(score)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8J4jRfsKqU0"
   },
   "source": [
    "Let's see now how we can leverage the cross-lingual word embeddings in order to perform zero-shot learning. A simple but effective baseline consists of averaging the word embeddings in each document in order to come up with a document (or sentence) representation. We will do that by using a look-up table in order to pull the appropriate cross-linual word embeddings for each document as it is shown in the diagram:\n",
    "\n",
    "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/vec_average.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRrsapJcKqU0"
   },
   "source": [
    "As we saw during the introduction we use a binary representation for the document terms which we use to perform a look-up in the embeddings matrix of size $V\\times d$, where $V$ is the size of the vocabulary and $d$ the dimension of the latent space, and pull the vectors. In the example we will pull three vectors. Finally, we will just calculate our document vector by just averaging the vectors. We will repeat this operation for each document in both the target and the source languages. Then we will follow the zero-shot learning framework and we will train a classifier on the source language and predict on the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGsGu2WOKqU_"
   },
   "outputs": [],
   "source": [
    "for name, myclf in zip(['Knn-nBow', 'LR-nBow'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
    "\n",
    "    avg_baseline = nBowClassifier(myclf,dataset.source_embeddings,dataset.target_embeddings)\n",
    "\n",
    "    pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True,vocabulary=dataset.vocab_)), \n",
    "                         ('classifier', avg_baseline)])\n",
    "\n",
    "    runner = Runner(pipeline, dataset)\n",
    "    x.add_row([name, format_score(runner.eval_system())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQTx_vhyKqVI"
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRzpaUeNKqVe"
   },
   "source": [
    "In the following model we will use the LASER representations in order to train the classifiers within the same framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GH3_g755KqVg"
   },
   "outputs": [],
   "source": [
    "for name, myclf in zip(['Knn-laser', 'LR-laser'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
    "    laser_clf = LASERClassifier(myclf, dataset.source_lang, dataset.target_lang)\n",
    "    pipeline = Pipeline([(\"doc2laser\",Doc2Laser()),('classifier', laser_clf)])\n",
    "    pipeline.set_params(doc2laser__lang=dataset.source_lang)\n",
    "    pipeline.fit(dataset.train,dataset.y_train)\n",
    "    runner = Runner(pipeline, dataset)\n",
    "\n",
    "    pipeline.set_params(doc2laser__lang=dataset.target_lang)\n",
    "    x.add_row([name, format_score(runner.eval_system(prefit=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyBme3i3KqVr"
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jamZepetKqVy"
   },
   "source": [
    "We observe that the zero-shot learning using LASER representations can achieve state-of-the-art results in this pair of languages. \n",
    "\n",
    "***Exercises:*** \n",
    "\n",
    "* Use other pairs of languages and see the performance. For example, you can try to transfer from more distant languages like Russian.\n",
    "* Write a function in order to calculate all the pairs of (source,target) languages and compare the results.\n",
    "* Tune the classifier or use other type of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMKJFj6NKqV1"
   },
   "source": [
    "# Few-shot Learning\n",
    "\n",
    "On this notebook, we will work on a multilingual dataset containing sentences in six languages: english, dutch, spanish, russian, arabic and turkish. Every sentence of every language comes along a with sentiment label indicating *positive* or *negative* content. There is no sentence overlap between idioms. \n",
    "\n",
    "Working with the LASER multilinguale representation, we directly provide the sentence embedding for all languages. Every sentence is represented by a 1024 dimensional vector indicating its position in LASER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6gF7VvHKqV4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "sys.path.insert(1, workdir)\n",
    "\n",
    "from src.utils import load_training_languages, model_evaluation, get_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldZjtvZdmEMz"
   },
   "source": [
    "The 3 following utility functions will be used in this notebook:\n",
    "\n",
    "- ```\n",
    "model_evaluation(model, [languages])\n",
    "```: evaluate the ```model``` over list of ```languages```. Returns [F1](https://en.wikipedia.org/wiki/F1_score) score, more suited for imbalanced dataset and [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to analyse model outputs in details.\n",
    "- ```x_train, y_train = load_training_languages([languages])```: Returns concatenated features and labels for languages specified in ```languages```.\n",
    "- ```get_statistics([languages]```: print out class population for languages specified in ```languages```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDGkNNCFKqWD"
   },
   "source": [
    "# Dataset statistics\n",
    "\n",
    "The multilingual dataset consists in 6 different languages: english (```en```), spanish (`es`), dutch (`nl`), russian (`ru`), arabic (`a`r) and turkish (`tr`).\n",
    "\n",
    "all_languages = ['en','es','nl','ru','ar','tr']\n",
    "\n",
    "get_statistics(all_languages)\n",
    "\n",
    "#Few Shot Learning\n",
    "While learning a language classification model generally requires abundance of training materials, it happens frequently that some languages are systematically under representated, leading to poor prediction performance. \n",
    "\n",
    "In that situation, using a common language representation such as LASER permits to increase the training data by adding to the initial (small) set, (possibly larger) dataset from other languages. \n",
    "\n",
    "As shown in figure below, poplulating the training space increases the chances to accurately determine the decision function.  \n",
    "\n",
    "![Few Shot Learning](https://upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png)\n",
    "\n",
    "In the following, we are going to experiment the Few Shot Learning concepts by training and testing classifier on different combinations of languague.\n",
    "\n",
    "Let's train a [Logistic Regression](https://fr.wikipedia.org/wiki/R%C3%A9gression_logistique) (a linear classifier) on russian, and look at the model accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBUVG8CnKqWG"
   },
   "outputs": [],
   "source": [
    "x_train,y_train = load_training_languages(['ru'])\n",
    "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
    "_ = model_evaluation(lr, ['ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78896zX5KqWV"
   },
   "source": [
    "The overall performance is not fantastic. Could we do better? Let's add more languages to the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_aDaev8KqWg"
   },
   "outputs": [],
   "source": [
    "x_train,y_train = load_training_languages(all_languages)\n",
    "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
    "_ = model_evaluation(lr, ['ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csU-_bJfKqWt"
   },
   "source": [
    "The F1 score has improved by 0.1! Quite impressive.\n",
    "\n",
    "Same operation with turkish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVW_rO_dKqWz"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train,y_train = load_training_languages(['tr'])\n",
    "lr = LogisticRegression(C = 10,random_state = 1).fit(x_train,y_train)\n",
    "_ = model_evaluation(lr, ['tr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3zZQnaNKqW9"
   },
   "source": [
    "The F1 score is now quite low. Small dataset, data quality, language complexity may explain the poor performance.\n",
    "\n",
    "Fair enough, let's use all available languages to improve our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5qs4LvaKqXA"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train,y_train = load_training_languages(all_languages)\n",
    "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
    "_ = model_evaluation(lr, ['tr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQf1QyeGKqXG"
   },
   "source": [
    "No improvement... Maybe another combination of languages leads to different results. What happen if we remove spanish and russian from the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnvlGoxyKqXH"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train,y_train = load_training_languages(['ar','tr','nl','en'])\n",
    "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
    "_ = model_evaluation(lr, ['tr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ARZF0deKqXK"
   },
   "source": [
    "Better! Apparently spanish and russian were perturbing the model for turkish language.\n",
    "\n",
    "Could we imagine a more systematic source language selection to optimize performance on a specific target language? (Beware that the test set of the target language cannot be used to perform this selection)\n",
    "\n",
    "## Non linear model\n",
    "\n",
    "Until now we have used Logisitic Regression. However more complex models, such as [multi layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pcPVEvNKqXK"
   },
   "outputs": [],
   "source": [
    " from sklearn.neural_network import MLPClassifier\n",
    " mlp = MLPClassifier(solver='lbfgs', \n",
    "                     hidden_layer_sizes=(16),\n",
    "                     activation = 'relu',\n",
    "                     alpha=1e-3,\n",
    "                     max_iter = 50,\n",
    "                     early_stopping =True,\n",
    "                     validation_fraction = 0.2, \n",
    "                     random_state=1)\\\n",
    "      \n",
    " _ = model_evaluation(mlp.fit(x_train,y_train),['ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWVM12v_RXG3"
   },
   "source": [
    "or [extreme gradient boosting](https://en.wikipedia.org/wiki/XGBoost) (xgboost) are obviously possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyiIGc05KqXR"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "boost = xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",max_depth =5, random_state=42)\n",
    "_ = model_evaluation(boost.fit(x_train,y_train),['ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvX_liTERnBB"
   },
   "source": [
    "What can we conclude from the above results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWFAeJ-pKqXV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "CL_NLP_full_notebook_for_collab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
